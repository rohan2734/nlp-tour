{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exciting-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "#loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "specified-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')\n",
    "#creating a object of model,\n",
    "#it holds the process text,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "looking-speaker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.S. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM pobj\n"
     ]
    }
   ],
   "source": [
    "#spacy, will parse the doc, into seperate components, called tokens(\n",
    "#each word is a token)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_,token.dep_)#token.pos (prints numbers for par\n",
    "    #ts of speech)\n",
    "    #dep_ , it will give more information, it is snytactic dependancy, we will\n",
    "    #see it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chief-trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla 96\n",
      "is 87\n",
      "looking 100\n",
      "at 85\n",
      "buying 100\n",
      "U.S. 96\n",
      "startup 92\n",
      "for 85\n",
      "$ 99\n",
      "6 93\n",
      "million 93\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text,token.pos_)\n",
    "    #for parts of speech, you can use underscore at end of pos, token.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "potential-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x2e6777f8450>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x2e6778439a0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x2e6777e5820>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x2e6777e5ac0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x2e677882880>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x2e6778be140>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline object\n",
    "nlp.pipeline\n",
    "#when we run our nlp,our text enters enters into  a processing pipeline,\n",
    "#that first breaks down our text.  and then it performs, seris of operations\n",
    "#the tok2vec,.. pr \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "molecular-rendering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "polish-bidding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "looking VERB ROOT\n",
      "for ADP prep\n",
      "startups NOUN pobj\n",
      "anymore ADV advmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "#tokenisation\n",
    "#for processing any texts, is to split up all the words or component parts into\n",
    "#tokens\n",
    "#and these tokens, are anootated, inside doc objects, to contain the \n",
    "#scripted information\n",
    "\n",
    "doc2 = nlp(u\"Tesla isn't looking for startups anymore.\")\n",
    "for token in doc2:\n",
    "    print(token.text,token.pos_,token.dep_)\n",
    "    \n",
    "#extended whitespace, will also become token\n",
    "#n't is also a token\n",
    "#. perios is also a token\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "heavy-walker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AUX'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[1].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "neither-creature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsubj'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "roman-scoop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-heating",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

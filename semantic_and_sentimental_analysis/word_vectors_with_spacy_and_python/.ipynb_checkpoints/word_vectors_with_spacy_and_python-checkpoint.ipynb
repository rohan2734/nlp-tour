{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "mobile-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download larger spacy english language models, that contain word vectors\n",
    "#smaller models , dont contain the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "available-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "sublime-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "revolutionary-section",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8963e-01, -4.0309e-01,  3.5350e-01, -4.7907e-01, -4.3311e-01,\n",
       "        2.3857e-01,  2.6962e-01,  6.4332e-02,  3.0767e-01,  1.3712e+00,\n",
       "       -3.7582e-01, -2.2713e-01, -3.5657e-01, -2.5355e-01,  1.7543e-02,\n",
       "        3.3962e-01,  7.4723e-02,  5.1226e-01, -3.9759e-01,  5.1333e-03,\n",
       "       -3.0929e-01,  4.8911e-02, -1.8610e-01, -4.1702e-01, -8.1639e-01,\n",
       "       -1.6908e-01, -2.6246e-01, -1.5983e-02,  1.2479e-01, -3.7276e-02,\n",
       "       -5.7125e-01, -1.6296e-01,  1.2376e-01, -5.5464e-02,  1.3244e-01,\n",
       "        2.7519e-02,  1.2592e-01, -3.2722e-01, -4.9165e-01, -3.5559e-01,\n",
       "       -3.0630e-01,  6.1185e-02, -1.6932e-01, -6.2405e-02,  6.5763e-01,\n",
       "       -2.7925e-01, -3.0450e-03, -2.2400e-02, -2.8015e-01, -2.1975e-01,\n",
       "       -4.3188e-01,  3.9864e-02, -2.2102e-01, -4.2693e-02,  5.2748e-02,\n",
       "        2.8726e-01,  1.2315e-01, -2.8662e-02,  7.8294e-02,  4.6754e-01,\n",
       "       -2.4589e-01, -1.1064e-01,  7.2250e-02, -9.4980e-02, -2.7548e-01,\n",
       "       -5.4097e-01,  1.2823e-01, -8.2408e-02,  3.1035e-01, -6.3394e-02,\n",
       "       -7.3755e-01, -5.4992e-01,  9.9999e-02, -2.0758e-01, -3.9674e-02,\n",
       "        2.0664e-01, -9.7557e-02, -3.7092e-01,  2.7901e-01, -6.2218e-01,\n",
       "       -1.0280e-01,  2.3271e-01,  4.3838e-01,  3.2445e-02, -2.9866e-01,\n",
       "       -7.3611e-02,  7.1594e-01,  1.4241e-01,  2.7770e-01, -3.9892e-01,\n",
       "        3.6656e-02,  1.5759e-01,  8.2014e-02, -5.7343e-01,  3.5457e-01,\n",
       "        2.2491e-01, -6.2699e-01, -8.8106e-02,  2.4361e-01,  3.8533e-01,\n",
       "       -1.4083e-01,  1.7691e-01,  7.0897e-02,  1.7951e-01, -4.5907e-01,\n",
       "       -8.2120e-01, -2.6631e-02,  6.2549e-02,  4.2415e-01, -8.9630e-02,\n",
       "       -2.4654e-01,  1.4156e-01,  4.0187e-01, -4.1232e-01,  8.4516e-02,\n",
       "       -1.0626e-01,  7.3145e-01,  1.9217e-01,  1.4240e-01,  2.8511e-01,\n",
       "       -2.9454e-01, -2.1948e-01,  9.0460e-01, -1.9098e-01, -1.0340e+00,\n",
       "       -1.5754e-01, -1.1964e-01,  4.9888e-01, -1.0624e+00, -3.2820e-01,\n",
       "       -1.1232e-02, -7.9482e-01,  3.7275e-01, -6.8710e-03, -2.5772e-01,\n",
       "       -4.7005e-01, -4.1387e-01, -6.4089e-02, -2.8033e-01, -4.0778e-02,\n",
       "       -2.4866e+00,  6.2494e-03, -1.0210e-02,  1.2752e-01,  3.4965e-01,\n",
       "       -1.2571e-01,  3.1570e-01,  4.1926e-01,  2.0056e-01, -5.5984e-01,\n",
       "       -2.2801e-01,  1.2012e-01, -2.0518e-03, -8.9764e-02, -8.0373e-02,\n",
       "        1.1969e-02, -2.6978e-01,  3.4829e-01,  7.3664e-03, -1.1137e-01,\n",
       "        6.3410e-01,  3.8449e-01, -6.2248e-01,  4.1145e-02,  2.5922e-01,\n",
       "        6.5811e-01, -4.9548e-01, -1.3030e-01, -3.8279e-01,  1.1156e-01,\n",
       "       -4.3085e-01,  3.4473e-01,  2.7109e-02, -2.5108e-01, -2.8011e-01,\n",
       "        2.1662e-01,  3.2660e-01,  5.5895e-02,  7.6077e-02, -5.2480e-02,\n",
       "        4.5928e-02, -2.5266e-01,  5.2845e-01, -1.3145e-01, -1.2453e-01,\n",
       "        4.0556e-01,  3.1877e-01,  2.4415e-02, -2.2620e-01, -6.1960e-01,\n",
       "       -4.0886e-01, -3.5534e-02, -5.5123e-03,  2.3438e-01,  8.7854e-01,\n",
       "       -2.5161e-01,  4.0600e-01, -4.4284e-01,  3.4934e-01, -5.6429e-01,\n",
       "       -2.3676e-01,  6.2199e-01, -2.8175e-01,  4.2024e-01,  1.0043e-01,\n",
       "       -1.4720e-01,  4.9593e-01, -3.5850e-01, -1.3998e-01, -2.7494e-01,\n",
       "        2.3827e-01,  5.7268e-01,  7.9025e-02,  1.7872e-02, -2.1829e-01,\n",
       "        5.5050e-02, -5.4200e-01,  1.6788e-01,  3.9065e-01,  3.0209e-01,\n",
       "        2.3040e-01, -3.9351e-02, -2.1078e-01, -2.7224e-01,  1.6907e-01,\n",
       "        5.4819e-01,  9.4888e-02,  7.9798e-01, -6.6158e-02,  1.9844e-01,\n",
       "        2.0307e-01,  4.4808e-02, -1.0240e-01, -6.9909e-02, -3.6756e-02,\n",
       "        9.5159e-02, -2.7830e-01, -1.0597e-01, -1.6276e-01, -1.8211e-01,\n",
       "       -3.1897e-01, -2.1633e-01,  1.4994e-01, -7.2057e-02,  2.2264e-01,\n",
       "       -4.5551e-01,  3.0341e-01,  1.8431e-01,  2.1681e-01, -3.1940e-01,\n",
       "        2.6426e-01,  5.8106e-01,  5.4635e-02,  6.3238e-01,  4.3169e-01,\n",
       "        9.0343e-02,  1.9494e-01,  3.5483e-01, -2.0706e-02, -7.3117e-01,\n",
       "        1.2941e-01,  1.7418e-01, -1.5065e-01,  5.3355e-02,  4.4794e-02,\n",
       "       -1.6600e-01,  2.2007e-01, -5.3970e-01, -2.4968e-01, -2.6464e-01,\n",
       "       -5.5515e-01,  5.8242e-01,  2.2295e-01,  2.4433e-01,  4.5275e-01,\n",
       "        3.4693e-01,  1.2255e-01, -3.9059e-02, -3.2749e-01, -2.7891e-01,\n",
       "        1.3766e-01,  3.8392e-01,  1.0543e-03, -1.0242e-02,  4.9205e-01,\n",
       "       -1.7922e-01,  4.1215e-02,  1.3547e-01, -2.0598e-01, -2.3194e-01,\n",
       "       -7.7701e-01, -3.8237e-01, -7.6383e-01,  1.9418e-01, -1.5441e-01,\n",
       "        8.9740e-01,  3.0626e-01,  4.0376e-01,  2.1738e-01, -3.8050e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u\"lion\").vector\n",
    "    #we are passing an unicode string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "acceptable-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc ans span objects also have vectors, and these vectors are originated from\n",
    "#the averages from the token vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "beneficial-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc itself is the average of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "painful-captain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u\"The quick brown fox jumped\").vector.shape\n",
    "#the doc is the avrage of all the singular vectors for all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "authorized-publisher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u\"fox\").vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "congressional-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the similar vectors\n",
    "#by . similarity method , on the document tokens\n",
    "#so we will create tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "concerned-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u\"lion cat pet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "military-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now there is similarity between cat and pet, cats are often pets\n",
    "# and lions and cat are also similar, since they are from same family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "collective-harbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.52654374\n",
      "lion pet 0.39923766\n",
      "cat lion 0.52654374\n",
      "cat cat 1.0\n",
      "cat pet 0.7505456\n",
      "pet lion 0.39923766\n",
      "pet cat 0.7505456\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text,token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "civic-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lion lion, cat cat , and pet pet, have 100% similarity b/w each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "printable-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat and pet have more similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "serious-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lion and pet, have less simiilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "apart-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u\"like love hate\")\n",
    "#we know that love and hate are having dfferent meanings, how ever, they are used in similar context like\n",
    "#we love a movie, or hate a movie, so in that aspect, they have some similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "statewide-beverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like like 1.0\n",
      "like love 0.65790397\n",
      "like hate 0.6574652\n",
      "love like 0.65790397\n",
      "love love 1.0\n",
      "love hate 0.6393099\n",
      "hate like 0.6574652\n",
      "hate love 0.6393099\n",
      "hate hate 1.0\n"
     ]
    }
   ],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text,token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "spare-sheep",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684830"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "empirical-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#they are 685 thousand unique words, in the vocabulary, that we have vectors for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "indian-airplane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(684830, 300)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "senior-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#685 thousand words,by 300 dimensions for each word vectora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "unique-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#an unusual name will not have a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "mighty-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u\"dog cat nargle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "organizational-compact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "nargle False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text,token.has_vector,token.vector_norm,token.is_oov)\n",
    "    #is__oov, out of vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "popular-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_norm ,sum of squares of 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "parallel-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common names also may be vectorised\n",
    "#we can acutally calculate a ne vector, from adding or subtracting the vectors\n",
    "# famous example is king - men +women = queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "direct-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we need to calculate cosine similarity ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "liquid-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial \n",
    "#~cipi === scipy\n",
    "\n",
    "cosine_similarity = lambda vec1,vec2 : 1 - spatial.distance.cosine(vec1,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "developmental-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "spoken-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are going to ask for king vector -man vector +woman vector ---->\n",
    "#NEW_VECTOR similar to Queen,princess,highness\n",
    "#we are assuming that , somewhere in that , 300 dimensions,\n",
    "#there might be some understanding that,, with the (royalty) king, (gender) man,\n",
    "#usually the kings are men, so subtracting (gender dimension)man from kings, \n",
    "#we might get some royalty and gender (woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "joined-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector = king-man+woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "controlling-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_similarities = []\n",
    "\n",
    "#For all words in my words\n",
    "for word in nlp.vocab:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:#not a number\n",
    "                similarity = cosine_similarity(new_vector, word.vector)\n",
    "                computed_similarities.append((word,similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "whole-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_similarities = sorted(computed_similarities,key=lambda item:-item[1])\n",
    "#- (descending order), (item[1] is similarity in (word,similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "breathing-terrorist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'woman', 'she', 'lion', 'who', 'fox', 'brown', 'when', 'dare', 'cat']\n"
     ]
    }
   ],
   "source": [
    "# computed_similarities[:10]\n",
    "\n",
    "print([t[0].text  for t in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-lightweight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
